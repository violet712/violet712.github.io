{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[智能决策论坛 | 张伟楠：Bidirectional Model-Based Policy Optimization](https://www.bilibili.com/video/BV1ZD4y1R7Ud/?vd_source=4b833ca277a66443288b1df0b65ac1d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-free RL v.s. Model-based RL\n",
    "\n",
    "Model-based RL:\n",
    "- Pros\n",
    "1. On-policy learning once the model is learned\n",
    "2. May not need further real interaction data once the model is learned (bacth RL)\n",
    "3. Always show higher sample efficiency than MFRL\n",
    "- Cons\n",
    "1. Suffer from model compounding error\n",
    "\n",
    "Model-free RL:\n",
    "- Pros\n",
    "1. The best asymptotic performance\n",
    "2. Highly suitable for DL architecture with big data\n",
    "\n",
    "- Cons\n",
    "1. Off-policy methods still show instabilities\n",
    "2. Very low sample efficiency & require huge amount of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRL的采样效率非常低, 深度强化学习是一种和环境相互交互过程中tiral and error learning的方式,所以它交互出来的数据将会作为接下来的训练数据. 但交互出来的数据不一定能够有效提升当前智能体的表现\n",
    "\n",
    "On-policy learning once the model is learned. 模型策略和环境策略一致,无需纠偏的操作. 而off-policy因为目标策略和行为策略不同,会使得真正与环境交互时的数据分布并不太一样(需要重要度采样)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2638d182245c591c0c7650a148f9b4622aaf18b8cff9e48ecf203e602cb4653c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
