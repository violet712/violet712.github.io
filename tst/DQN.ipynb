{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gym\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_ACTIONS = [0, 1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        self.input_state = tf.keras.Input(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "        self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "        self.output = tf.image.resize_images(self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(keras.layers.Layer):\n",
    "    '''\n",
    "    Q-value estimator neural network\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, summaries_dir=None):\n",
    "        self.summary_writer = None\n",
    "\n",
    "    def build_model(self,):\n",
    "        self.X_pl = tf.keras.Input(\n",
    "            shape=[None, 84, 84, 4], dtype=tf.uint8, name='X')\n",
    "        self.y_pl = tf.keras.Input(\n",
    "            shape=[None], dtype=tf.float32, name='y')\n",
    "        self.actions_pl = tf.keras.Input(\n",
    "            shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = self.X_pl / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32, kernel_size=8, strides=4, activation='relu')(X)\n",
    "        conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64, kernel_size=4, strides=2, activation='relu')(conv1)\n",
    "        conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=64, kernel_size=3, strides=1, activation='relu')(conv2)\n",
    "\n",
    "        flattened = tf.keras.layers.Flatten()(conv3)\n",
    "        fc1 = tf.keras.layers.Dense(512)(flattened)\n",
    "        self.predictions = tf.keras.Dense(len(VALID_ACTIONS))(fc1)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2638d182245c591c0c7650a148f9b4622aaf18b8cff9e48ecf203e602cb4653c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
