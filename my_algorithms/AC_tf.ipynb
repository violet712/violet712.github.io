{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import rl_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(gym.__version__)\n",
    "\n",
    "logdir = 'logs/scalars'\n",
    "file_writer = tf.summary.create_file_writer(logdir + \"/AC_tf\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "# tf.summary.trace_on(graph=True)\n",
    "\n",
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-2 # critic learning rate is faster than actor\n",
    "num_episodes = 1000\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_layer = keras.Input(shape=(state_dim))\n",
    "common_layer = layers.Dense(128, activation='relu')(inputs_layer)\n",
    "actor_layer = layers.Dense(action_dim, activation='softmax')(common_layer)\n",
    "critic_layer = layers.Dense(1, activation='relu')(common_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, actor_lr, critic_lr, gamma) -> None:\n",
    "        self.actor = keras.Model(inputs=inputs_layer, outputs=actor_layer)\n",
    "        self.critic = keras.Model(inputs=inputs_layer, outputs=critic_layer)\n",
    "        \n",
    "        self.actor_optimizer = keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "        self.critic_optimizer = keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        state = tf.constant([state], dtype=tf.float32)\n",
    "        probs = self.actor(state)\n",
    "        # print(probs)\n",
    "        action = tf.random.categorical(probs, 1)[0][0].numpy()\n",
    "        return action\n",
    "    \n",
    "    def update(self, transition_dict):\n",
    "        states = tf.constant(transition_dict['states'], dtype=tf.float32)\n",
    "        actions = tf.reshape(tf.constant(transition_dict['actions'], dtype=tf.int32), [-1, 1])\n",
    "        rewards = tf.reshape(tf.constant(transition_dict['rewards'], dtype=tf.float32), [-1, 1])\n",
    "        next_states = tf.constant(transition_dict['next_states'], dtype=tf.float32)\n",
    "        dones = tf.reshape(tf.constant(transition_dict['dones'], dtype=tf.float32), [-1, 1])\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)\n",
    "            td_error = td_target - self.critic(states)\n",
    "            #?\n",
    "            probs = tf.gather(self.actor(states), actions, axis=1)\n",
    "            log_probs = tf.math.log(probs)\n",
    "            actor_loss = tf.reduce_mean(-log_probs * td_error)\n",
    "            # mse = tf.keras.losses.MeanSquaredError(reduction='mean')\n",
    "            # critic_loss = mse(td_target, self.critic(states))\n",
    "            critic_loss = tf.reduce_mean(tf.losses.MSE(td_target, self.critic(states)))\n",
    "        \n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grad, self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))   \n",
    "        \n",
    "        del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ActorCritic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\vscodespace\\violet712.github.io\\my_algorithms\\AC_tf.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/vscodespace/violet712.github.io/my_algorithms/AC_tf.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m ActorCritic(actor_lr, critic_lr, gamma)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/vscodespace/violet712.github.io/my_algorithms/AC_tf.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m return_list \u001b[39m=\u001b[39m rl_utils\u001b[39m.\u001b[39mtrain_on_policy_agent(env, agent, num_episodes)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ActorCritic' is not defined"
     ]
    }
   ],
   "source": [
    "agent = ActorCritic(actor_lr, critic_lr, gamma)\n",
    "\n",
    "return_list = rl_utils.train_on_policy_agent(env, agent, num_episodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_list = []\n",
    "\n",
    "# for i in range(10):\n",
    "#     for i_episode in range(int(num_episodes/10)):\n",
    "#         episode_return = 0\n",
    "#         transition_dict = {'states': [], 'actions': [],\n",
    "#                                 'next_states': [], 'rewards': [], 'dones': []}\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             action = agent.take_action(state)\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "#             transition_dict['states'].append(state)\n",
    "#             transition_dict['actions'].append(action)\n",
    "#             transition_dict['next_states'].append(next_state)\n",
    "#             transition_dict['rewards'].append(reward)\n",
    "#             transition_dict['dones'].append(done)\n",
    "#             state = next_state\n",
    "#             episode_return += reward\n",
    "#         return_list.append(episode_return)\n",
    "#         agent.update(transition_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Actor-Critic on {}'.format(env_name))\n",
    "plt.show()\n",
    "\n",
    "mv_return = rl_utils.moving_average(return_list, 9)\n",
    "plt.plot(episodes_list, mv_return)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Actor-Critic on {}'.format(env_name))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "a1= np.array(a)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "print(tf.random.uniform([1]))  # generates 'A1'\n",
    "print(tf.random.uniform([1]))  # generates 'A2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.constant(1.0)\n",
    "print(s.dtype)\n",
    "s =s + 1\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = tf.constant([state], dtype=tf.float32)\n",
    "print(state)\n",
    "state = state + 1.0\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dict = {'states': []}\n",
    "s_dict['states'].append(env.reset())\n",
    "state, _, _, _ = env.step(0)\n",
    "s_dict['states'].append(state)\n",
    "\n",
    "print(s_dict['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = keras.Model(inputs=inputs_layer, outputs=actor_layer)\n",
    "\n",
    "# s = tf.constant([env.reset()], dtype=tf.float32)\n",
    "# a = actor_net(s)\n",
    "# print(s.shape)\n",
    "# print(a)\n",
    "# states = s_dict['states']\n",
    "# for i in range(len(states)):\n",
    "#     actor_net([1, states[i]])\n",
    "s1 = tf.constant(s_dict['states'], dtype=tf.float32) # shape = (2, 4)\n",
    "print(s1)\n",
    "a1 = actor_net(s1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([0.0])\n",
    "with tf.GradientTape() as g:\n",
    "    loss = tf.constant(v + v)\n",
    "g.gradient(loss, v).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.constant([0.0])\n",
    "with tf.GradientTape() as g:\n",
    "    loss = (q + q)\n",
    "print(g.gradient(loss, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = tf.Variable([0.0])\n",
    "q2 = q1 + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q1)\n",
    "print(q2)\n",
    "print(q2.numpy())\n",
    "print(q2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant(0.0)\n",
    "x1 = tf.constant(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\n",
    "  tape0.watch(x0)\n",
    "  tape1.watch(x1)\n",
    "\n",
    "  y0 = tf.math.sin(x0)\n",
    "  y1 = tf.nn.sigmoid(x1)\n",
    "\n",
    "  y = y0 + y1\n",
    "\n",
    "  ys = tf.reduce_sum(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1. ,1], [0., 0. ,1]]\n",
    "y_pred = [[1., 1. ,1], [1., 0., 1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "mse = tf.keras.losses.MeanSquaredError(reduction='none')\n",
    "mse(y_true, y_pred).numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2638d182245c591c0c7650a148f9b4622aaf18b8cff9e48ecf203e602cb4653c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
